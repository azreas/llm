{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23160e39-87be-4f55-9cda-8156b3fe7050",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec122fbbad234e4c8ce904c28ad5212e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLMForConditionalGeneration(\n",
      "  (transformer): ChatGLMModel(\n",
      "    (embedding): Embedding(\n",
      "      (word_embeddings): Embedding(65024, 4096)\n",
      "    )\n",
      "    (rotary_pos_emb): RotaryEmbedding()\n",
      "    (encoder): GLMTransformer(\n",
      "      (layers): ModuleList(\n",
      "        (0-27): 28 x GLMBlock(\n",
      "          (input_layernorm): RMSNorm()\n",
      "          (self_attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n",
      "            (core_attention): CoreAttention(\n",
      "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (post_attention_layernorm): RMSNorm()\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
      "            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layernorm): RMSNorm()\n",
      "    )\n",
      "    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGLMForConditionalGeneration(\n",
       "  (transformer): ChatGLMModel(\n",
       "    (embedding): Embedding(\n",
       "      (word_embeddings): Embedding(65024, 4096)\n",
       "    )\n",
       "    (rotary_pos_emb): RotaryEmbedding()\n",
       "    (encoder): GLMTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x GLMBlock(\n",
       "          (input_layernorm): RMSNorm()\n",
       "          (self_attention): SelfAttention(\n",
       "            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n",
       "            (core_attention): CoreAttention(\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (post_attention_layernorm): RMSNorm()\n",
       "          (mlp): MLP(\n",
       "            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
       "            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layernorm): RMSNorm()\n",
       "    )\n",
       "    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import PeftModel\n",
    "# import random\n",
    "# random.seed(42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/opt/cache/modelscope/hub/ZhipuAI/chatglm3-6b\",trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"/opt/cache/modelscope/hub/ZhipuAI/chatglm3-6b\",trust_remote_code=True).half().cuda()\n",
    "# print(model)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfac752a-d94a-469b-979e-418824b0bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "new_model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            \"/opt/data/chatglm3-6b/customer_sft_300_all_multi\",\n",
    "            device_map='auto'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362f6f2c-8576-4146-b045-a98deca0091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎光临新华文轩旗舰店！很高兴为您服务！\n",
      "这个您根据您小孩所学课本的版本选择对应的版本就可以的，这个书是和教材同步的，您只要选对版本就是和您所在地的课本是同步的\n",
      "是有答案和解析的哈，您扫书上的二维码就可以的\n"
     ]
    }
   ],
   "source": [
    "response, history = new_model.chat(tokenizer, \"你好\", history=[])\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"我刚才买的书买重了，怎么退款\", history=history)\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"但是我看到书已经在配送了，你们拦截一下\", history=history)\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"行，反正你们记得拦截哈\", history=history)\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"好\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb9fa76-e8b8-48a9-927e-f092e48741ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = new_model.chat(tokenizer, \"老板在吗？\", history=[])\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"我买多了，能不能退货？\", history=history)\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"在哪儿申请\", history=history)\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"好\", history=history)\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"我已经申请退货了，你们赶紧审核\", history=history)\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"行\", history=history)\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"没了，赶紧审核就行了\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c164ff-5f20-4858-8678-3d51c37e6963",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = new_model.chat(tokenizer, \"老板在吗？\", history=[])\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"我想买10本，价格可以优惠点吗？\", history=history)\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"给我优惠点嘛，我买那么多\", history=history)\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"那有没有什么优惠券呢？\", history=history)\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"那我看看\", history=history)\n",
    "print(response)\n",
    "response, history = new_model.chat(tokenizer, \"我看到有减10块的券\", history=history)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
